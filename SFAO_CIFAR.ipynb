{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbae30d5-e028-45e6-ae4a-8ea52d10e6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.7/109.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
      "Downloading matplotlib-3.10.6-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m187.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m189.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.59.2 kiwisolver-1.4.9 matplotlib-3.10.6 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib numpy torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb71e4bf-fdb0-421c-9c37-86728be55b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Task 0: classes 0-9, train=5000, test=1000\n",
      "Task 1: classes 10-19, train=5000, test=1000\n",
      "Task 2: classes 20-29, train=5000, test=1000\n",
      "Task 3: classes 30-39, train=5000, test=1000\n",
      "Task 4: classes 40-49, train=5000, test=1000\n",
      "Task 5: classes 50-59, train=5000, test=1000\n",
      "Task 6: classes 60-69, train=5000, test=1000\n",
      "Task 7: classes 70-79, train=5000, test=1000\n",
      "Task 8: classes 80-89, train=5000, test=1000\n",
      "Task 9: classes 90-99, train=5000, test=1000\n",
      "Prepared 10 tasks (mode=split)\n",
      "\n",
      "=== Training Task 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 20.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=2.1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=1.9636\n",
      "Accuracy on Task 1: 0.403\n",
      "\n",
      "=== Training Task 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:23<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=2.2129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:23<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=1.9684\n",
      "Accuracy on Task 1: 0.150\n",
      "Accuracy on Task 2: 0.363\n",
      "\n",
      "=== Training Task 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=2.1258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=1.8786\n",
      "Accuracy on Task 1: 0.134\n",
      "Accuracy on Task 2: 0.158\n",
      "Accuracy on Task 3: 0.416\n",
      "\n",
      "=== Training Task 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=2.0399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=1.7812\n",
      "Accuracy on Task 1: 0.099\n",
      "Accuracy on Task 2: 0.195\n",
      "Accuracy on Task 3: 0.140\n",
      "Accuracy on Task 4: 0.397\n",
      "\n",
      "=== Training Task 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=1.9814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=1.6789\n",
      "Accuracy on Task 1: 0.088\n",
      "Accuracy on Task 2: 0.118\n",
      "Accuracy on Task 3: 0.105\n",
      "Accuracy on Task 4: 0.143\n",
      "Accuracy on Task 5: 0.494\n",
      "\n",
      "=== Training Task 6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=1.8632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=1.5416\n",
      "Accuracy on Task 1: 0.071\n",
      "Accuracy on Task 2: 0.072\n",
      "Accuracy on Task 3: 0.097\n",
      "Accuracy on Task 4: 0.043\n",
      "Accuracy on Task 5: 0.224\n",
      "Accuracy on Task 6: 0.517\n",
      "\n",
      "=== Training Task 7 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=1.8558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=1.4787\n",
      "Accuracy on Task 1: 0.108\n",
      "Accuracy on Task 2: 0.066\n",
      "Accuracy on Task 3: 0.065\n",
      "Accuracy on Task 4: 0.069\n",
      "Accuracy on Task 5: 0.163\n",
      "Accuracy on Task 6: 0.116\n",
      "Accuracy on Task 7: 0.507\n",
      "\n",
      "=== Training Task 8 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=1.9442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=1.5754\n",
      "Accuracy on Task 1: 0.198\n",
      "Accuracy on Task 2: 0.073\n",
      "Accuracy on Task 3: 0.066\n",
      "Accuracy on Task 4: 0.094\n",
      "Accuracy on Task 5: 0.117\n",
      "Accuracy on Task 6: 0.152\n",
      "Accuracy on Task 7: 0.086\n",
      "Accuracy on Task 8: 0.519\n",
      "\n",
      "=== Training Task 9 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=1.8414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=1.5320\n",
      "Accuracy on Task 1: 0.051\n",
      "Accuracy on Task 2: 0.080\n",
      "Accuracy on Task 3: 0.077\n",
      "Accuracy on Task 4: 0.074\n",
      "Accuracy on Task 5: 0.084\n",
      "Accuracy on Task 6: 0.165\n",
      "Accuracy on Task 7: 0.132\n",
      "Accuracy on Task 8: 0.185\n",
      "Accuracy on Task 9: 0.520\n",
      "\n",
      "=== Training Task 10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=1.7629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:44<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=1.4150\n",
      "Accuracy on Task 1: 0.100\n",
      "Accuracy on Task 2: 0.036\n",
      "Accuracy on Task 3: 0.090\n",
      "Accuracy on Task 4: 0.069\n",
      "Accuracy on Task 5: 0.053\n",
      "Accuracy on Task 6: 0.080\n",
      "Accuracy on Task 7: 0.197\n",
      "Accuracy on Task 8: 0.150\n",
      "Accuracy on Task 9: 0.171\n",
      "Accuracy on Task 10: 0.574\n",
      "=== Continual Learning Metrics ===\n",
      "Average Accuracy (ACC):       0.1520\n",
      "Forgetting (F):              0.3544\n",
      "Backward Transfer (BWT):     -0.3544\n",
      "Forward Transfer (FWT):      0.0000\n",
      "Memory Usage:                27831.42 MB\n",
      "Computation Cost:            200 projections/batch\n",
      "Plasticity-Stability Measure (PSM): 0.3228 (0-1 normalized)\n",
      "=== Metrics (Permuted MNIST) ===\n",
      "ACC=0.1520, F=0.3544, BWT=-0.3544, FWT=0.0000, Mem=27831.42MB, PSM=0.3228\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell: prepare CIFAR-100 tasks for SFAO (split or permuted) with CNN ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Hyperparameters ===\n",
    "root = './data'\n",
    "num_tasks = 10\n",
    "num_classes = 100\n",
    "classes_per_task = num_classes // num_tasks\n",
    "batch_size = 32\n",
    "download = True\n",
    "mode = 'split'   # 'split' or 'permuted'\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# CIFAR-100 mean/std for normalization\n",
    "mean = torch.tensor([0.5071, 0.4867, 0.4408]).view(3, 1, 1)\n",
    "std = torch.tensor([0.2675, 0.2565, 0.2761]).view(3, 1, 1)\n",
    "\n",
    "# === Transforms ===\n",
    "train_transform = transforms.ToTensor()\n",
    "test_transform = transforms.ToTensor()\n",
    "\n",
    "# === Load datasets ===\n",
    "train_ds = datasets.CIFAR100(root=root, train=True, download=download, transform=train_transform)\n",
    "test_ds = datasets.CIFAR100(root=root, train=False, download=download, transform=test_transform)\n",
    "\n",
    "# === Helper: extract subset tensors for a given set of class IDs ===\n",
    "def extract_subset_tensors(dataset, class_list):\n",
    "    targets = np.array(dataset.targets)\n",
    "    mask = np.isin(targets, class_list)\n",
    "    indices = np.nonzero(mask)[0].tolist()\n",
    "    imgs, labs = [], []\n",
    "    for i in indices:\n",
    "        img, lbl = dataset[i]\n",
    "        imgs.append(img)\n",
    "        labs.append(lbl)\n",
    "    return torch.stack(imgs), torch.tensor(labs, dtype=torch.long)\n",
    "\n",
    "# === Prepare permutations if needed ===\n",
    "permutations = []\n",
    "if mode == 'permuted':\n",
    "    rng = np.random.default_rng(42)\n",
    "    for t in range(num_tasks):\n",
    "        perm = rng.permutation(3 * 32 * 32).astype(np.int64)\n",
    "        permutations.append(torch.tensor(perm, dtype=torch.long))\n",
    "\n",
    "# === Build tasks ===\n",
    "train_tasks, test_tasks = [], []\n",
    "for t in range(num_tasks):\n",
    "    cls_start = t * classes_per_task\n",
    "    cls_end = cls_start + classes_per_task\n",
    "    task_classes = list(range(cls_start, cls_end))\n",
    "\n",
    "    x_train, y_train = extract_subset_tensors(train_ds, task_classes)\n",
    "    x_test, y_test = extract_subset_tensors(test_ds, task_classes)\n",
    "\n",
    "    # Apply permutation if required\n",
    "    if mode == 'permuted':\n",
    "        perm = permutations[t]\n",
    "        N_train = x_train.shape[0]\n",
    "        x_train = x_train.view(N_train, -1)[:, perm].view(N_train, 3, 32, 32)\n",
    "        N_test = x_test.shape[0]\n",
    "        x_test = x_test.view(N_test, -1)[:, perm].view(N_test, 3, 32, 32)\n",
    "\n",
    "    # Normalize\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    # Map labels to 0..(classes_per_task-1)\n",
    "    class_map = {orig: i for i, orig in enumerate(task_classes)}\n",
    "    y_train_mapped = torch.tensor([class_map[int(v)] for v in y_train])\n",
    "    y_test_mapped = torch.tensor([class_map[int(v)] for v in y_test])\n",
    "\n",
    "    # Store datasets (no flattening — CNN expects 3x32x32)\n",
    "    train_tasks.append(TensorDataset(x_train, y_train_mapped))\n",
    "    test_tasks.append(TensorDataset(x_test, y_test_mapped))\n",
    "\n",
    "    print(f\"Task {t}: classes {task_classes[0]}-{task_classes[-1]}, train={len(x_train)}, test={len(x_test)}\")\n",
    "\n",
    "print(f\"Prepared {len(train_tasks)} tasks (mode={mode})\")\n",
    "\n",
    "# === CNN model ===\n",
    "\"\"\"\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x16x16\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 64x8x8\n",
    "        x = x.view(x.size(0), -1)             # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\"\"\"\n",
    "\n",
    "# === WRN model ===\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, 3, stride, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, 3, 1, 1, bias=False)\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = None if self.equalInOut else nn.Conv2d(\n",
    "            in_planes, out_planes, 1, stride, 0, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(x))\n",
    "        shortcut = x if self.equalInOut else self.convShortcut(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.relu2(self.bn2(out))\n",
    "        out = self.conv2(out)\n",
    "        return out + shortcut\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, n, in_planes, out_planes, block, stride):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(\n",
    "                block(in_planes if i == 0 else out_planes,\n",
    "                      out_planes,\n",
    "                      stride if i == 0 else 1)\n",
    "            )\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth=28, widen_factor=10, num_classes=10):\n",
    "        super().__init__()\n",
    "        assert (depth - 4) % 6 == 0\n",
    "        n = (depth - 4) // 6\n",
    "        k = widen_factor\n",
    "        nChannels = [16, 16*k, 32*k, 64*k]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], 3, 1, 1, bias=False)\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], BasicBlock, 1)\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], BasicBlock, 2)\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], BasicBlock, 2)\n",
    "        self.bn = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.relu(self.bn(x))\n",
    "        x = F.adaptive_avg_pool2d(x, 1).view(-1, self.nChannels)\n",
    "        return self.fc(x)\n",
    "\n",
    "# === SFAO class ===\n",
    "# SFAO Implementation (Updated Methods Only)\n",
    "class SFAO:\n",
    "    def __init__(self, model, lr=0.001, device='cuda',\n",
    "                 max_memory=200, cosine_threshold=0.85, discard_threshold=-1e-4, sample_size=750):\n",
    "        self.model = model.to(device)\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.S = []  # stored gradient vectors\n",
    "        self.max_memory = max_memory\n",
    "        self.cosine_threshold = cosine_threshold\n",
    "        self.discard_threshold = discard_threshold\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    # flatten gradient\n",
    "    def _flatten_grad(self):\n",
    "        return torch.cat([p.grad.view(-1) for p in self.model.parameters() if p.grad is not None])\n",
    "\n",
    "    # assign gradient\n",
    "    def _assign_grad(self, flat_grad):\n",
    "        idx = 0\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                numel = p.numel()\n",
    "                p.grad.copy_(flat_grad[idx:idx+numel].view_as(p))\n",
    "                idx += numel\n",
    "\n",
    "    # project new gradient against memory\n",
    "    def _project_grad(self, g):\n",
    "        if len(self.S) == 0: return g\n",
    "        # sample some stored gradients\n",
    "        sampled = self.S if len(self.S)<=self.sample_size else [self.S[i] for i in torch.randperm(len(self.S))[:self.sample_size]]\n",
    "        max_cos = 0.0\n",
    "        for v in sampled:\n",
    "            cos_sim = torch.dot(g, v)/(torch.norm(g)*torch.norm(v)+1e-12)\n",
    "            max_cos = max(max_cos, abs(cos_sim.item()))\n",
    "           #max_cos = max(max_cos, cos_sim.item())\n",
    "        if max_cos > self.cosine_threshold:\n",
    "            return g  # do not project\n",
    "        if max_cos < self.discard_threshold:\n",
    "            return None\n",
    "        g_proj = g.clone()\n",
    "        for v in sampled:\n",
    "            proj = (torch.dot(g_proj, v)/torch.dot(v,v))*v\n",
    "            g_proj -= proj\n",
    "        # discard tiny projected gradients\n",
    "        if torch.norm(g_proj) < self.discard_threshold:\n",
    "            return None\n",
    "        return g_proj\n",
    "\n",
    "    # one training step\n",
    "    def observe(self, x, y, criterion=nn.CrossEntropyLoss()):\n",
    "        self.model.train()\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = criterion(self.model(x), y)\n",
    "        loss.backward()\n",
    "\n",
    "        g = self._flatten_grad()\n",
    "        g_proj = self._project_grad(g)\n",
    "        if g_proj is not None:\n",
    "            self._assign_grad(g_proj)\n",
    "            self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    # store task gradients\n",
    "    def end_task(self, dataloader, criterion=nn.CrossEntropyLoss()):\n",
    "        self.model.eval()\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            self.model.zero_grad()\n",
    "            loss = criterion(self.model(x), y)\n",
    "            loss.backward()\n",
    "            g = self._flatten_grad()\n",
    "            norm_g = g / (torch.norm(g)+1e-10)\n",
    "            if len(self.S) >= self.max_memory:\n",
    "                self.S.pop(0)\n",
    "            self.S.append(norm_g.detach().clone())\n",
    "\n",
    "# === Training loop ===\n",
    "sfao = SFAO(WideResNet(depth=28, widen_factor=10, num_classes=classes_per_task),\n",
    "            lr=learning_rate, device=device)\n",
    "# sfao = SFAO(SimpleCNN(num_classes=classes_per_task), lr=learning_rate, device=device)\n",
    "accuracy_matrix = np.zeros((num_tasks, num_tasks))\n",
    "\n",
    "def evaluate_task(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x).argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "for task_id, train_dataset in enumerate(train_tasks):\n",
    "    print(f\"\\n=== Training Task {task_id+1} ===\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for x, y in tqdm(train_loader):\n",
    "            epoch_loss += sfao.observe(x, y)\n",
    "        print(f\"Epoch {epoch+1}, Loss={epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate on all seen tasks\n",
    "    for eval_id in range(task_id+1):\n",
    "        test_loader = DataLoader(test_tasks[eval_id], batch_size=batch_size, shuffle=False)\n",
    "        acc = evaluate_task(sfao.model, test_loader)\n",
    "        accuracy_matrix[task_id, eval_id] = acc\n",
    "        print(f\"Accuracy on Task {eval_id+1}: {acc:.3f}\")\n",
    "\n",
    "    sfao.end_task(train_loader)\n",
    "\n",
    "# Compute Continual Learning Metrics\n",
    "# Average Accuracy (ACC)\n",
    "ACC = accuracy_matrix[-1].mean()  # Final row averaged across tasks\n",
    "\n",
    "# Average Forgetting (F)\n",
    "F = np.mean([\n",
    "    np.max(accuracy_matrix[:num_tasks-1, j]) - accuracy_matrix[-1, j]\n",
    "    for j in range(num_tasks-1)\n",
    "])\n",
    "\n",
    "# Backward Transfer (BWT)\n",
    "BWT = np.mean([\n",
    "    accuracy_matrix[-1, j] - accuracy_matrix[j, j]\n",
    "    for j in range(num_tasks-1)\n",
    "])\n",
    "\n",
    "# Forward Transfer (FWT)\n",
    "# Measures how much previous tasks helped the next task before it was trained\n",
    "FWT = np.mean([\n",
    "    accuracy_matrix[i, i+1]\n",
    "    for i in range(num_tasks-1)\n",
    "])\n",
    "\n",
    "# Memory Usage (in MB)\n",
    "num_params = sum(p.numel() for p in sfao.model.parameters())\n",
    "mem_usage = len(sfao.S) * num_params * 4 / (1024**2)  # 4 bytes per float32\n",
    "\n",
    "# 6. Computation Cost (Number of gradient projections per batch)\n",
    "comp_cost = len(sfao.S)\n",
    "\n",
    "# Plasticity-Stability Measure (PSM) - Normalized 0 to 1\n",
    "\n",
    "# Define components\n",
    "stability = 1 - F                 # High if forgetting is low\n",
    "plasticity = max(FWT, 0)          # High if positive forward transfer\n",
    "\n",
    "alpha = 0.5                       # Balance between stability and plasticity\n",
    "PSM = alpha * stability + (1 - alpha) * plasticity\n",
    "\n",
    "# Print Metrics\n",
    "print(\"=== Continual Learning Metrics ===\")\n",
    "print(f\"Average Accuracy (ACC):       {ACC:.4f}\")\n",
    "print(f\"Forgetting (F):              {F:.4f}\")\n",
    "print(f\"Backward Transfer (BWT):     {BWT:.4f}\")\n",
    "print(f\"Forward Transfer (FWT):      {FWT:.4f}\")\n",
    "print(f\"Memory Usage:                {mem_usage:.2f} MB\")\n",
    "print(f\"Computation Cost:            {comp_cost} projections/batch\")\n",
    "print(f\"Plasticity-Stability Measure (PSM): {PSM:.4f} (0-1 normalized)\")\n",
    "\n",
    "print(\"=== Metrics (Split CIFAR-100 SFAO) ===\")\n",
    "print(f\"ACC={ACC:.4f}, F={F:.4f}, BWT={BWT:.4f}, FWT={FWT:.4f}, Mem={mem_usage:.2f}MB, PSM={PSM:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(accuracy_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Accuracy')\n",
    "plt.xlabel('Evaluation Task')\n",
    "plt.ylabel('Training Task')\n",
    "plt.title('SFAO Accuracy Matrix (Split CIFAR-100 SFAO)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f1f596-b22b-4c2c-b1e3-01388122fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Task 0: classes 0-1, train=10000, test=2000\n",
      "Task 1: classes 2-3, train=10000, test=2000\n",
      "Task 2: classes 4-5, train=10000, test=2000\n",
      "Task 3: classes 6-7, train=10000, test=2000\n",
      "Task 4: classes 8-9, train=10000, test=2000\n",
      "Prepared 5 tasks (mode=split)\n",
      "\n",
      "=== Training Task 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:14<00:00, 21.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=0.4859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:14<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss=0.3556\n",
      "Accuracy on Task 1: 0.877\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 44.45 GiB of which 10.00 MiB is free. Process 2333033 has 24.71 GiB memory in use. Process 2338752 has 1.56 GiB memory in use. Process 2365016 has 18.16 GiB memory in use. Of the allocated memory 17.03 GiB is allocated by PyTorch, and 804.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 276\u001b[0m\n\u001b[1;32m    273\u001b[0m         accuracy_matrix[task_id, eval_id] \u001b[38;5;241m=\u001b[39m acc\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy on Task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m     \u001b[43msfao\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# === Metrics ===\u001b[39;00m\n\u001b[1;32m    279\u001b[0m ACC \u001b[38;5;241m=\u001b[39m accuracy_matrix[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m, in \u001b[0;36mSFAO.end_task\u001b[0;34m(self, dataloader, criterion)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     76\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x), y)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_grad()\n\u001b[1;32m     79\u001b[0m norm_g \u001b[38;5;241m=\u001b[39m g \u001b[38;5;241m/\u001b[39m (torch\u001b[38;5;241m.\u001b[39mnorm(g)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-10\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 44.45 GiB of which 10.00 MiB is free. Process 2333033 has 24.71 GiB memory in use. Process 2338752 has 1.56 GiB memory in use. Process 2365016 has 18.16 GiB memory in use. Of the allocated memory 17.03 GiB is allocated by PyTorch, and 804.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Split CIFAR-10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class SFAO:\n",
    "    def __init__(self, model, lr=0.001, device='cuda',\n",
    "                 max_memory=200, cosine_threshold=0.85, discard_threshold=-1e-4, sample_size=750):\n",
    "        self.model = model.to(device)\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.S = []  # stored gradient vectors\n",
    "        self.max_memory = max_memory\n",
    "        self.cosine_threshold = cosine_threshold\n",
    "        self.discard_threshold = discard_threshold\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    # flatten gradient\n",
    "    def _flatten_grad(self):\n",
    "        return torch.cat([p.grad.view(-1) for p in self.model.parameters() if p.grad is not None])\n",
    "\n",
    "    # assign gradient\n",
    "    def _assign_grad(self, flat_grad):\n",
    "        idx = 0\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                numel = p.numel()\n",
    "                p.grad.copy_(flat_grad[idx:idx+numel].view_as(p))\n",
    "                idx += numel\n",
    "\n",
    "    # project new gradient against memory\n",
    "    def _project_grad(self, g):\n",
    "        if len(self.S) == 0: return g\n",
    "        # sample some stored gradients\n",
    "        sampled = self.S if len(self.S)<=self.sample_size else [self.S[i] for i in torch.randperm(len(self.S))[:self.sample_size]]\n",
    "        max_cos = 0.0\n",
    "        for v in sampled:\n",
    "            cos_sim = torch.dot(g, v)/(torch.norm(g)*torch.norm(v)+1e-12)\n",
    "            max_cos = max(max_cos, abs(cos_sim.item()))\n",
    "        if max_cos > self.cosine_threshold:\n",
    "            return g  # do not project\n",
    "        if max_cos < self.discard_threshold:\n",
    "            return None\n",
    "        g_proj = g.clone()\n",
    "        for v in sampled:\n",
    "            proj = (torch.dot(g_proj, v)/torch.dot(v,v))*v\n",
    "            g_proj -= proj\n",
    "        # discard tiny projected gradients\n",
    "        if torch.norm(g_proj) < self.discard_threshold:\n",
    "            return None\n",
    "        return g_proj\n",
    "\n",
    "    # one training step\n",
    "    def observe(self, x, y, criterion=nn.CrossEntropyLoss()):\n",
    "        self.model.train()\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = criterion(self.model(x), y)\n",
    "        loss.backward()\n",
    "\n",
    "        g = self._flatten_grad()\n",
    "        g_proj = self._project_grad(g)\n",
    "        if g_proj is not None:\n",
    "            self._assign_grad(g_proj)\n",
    "            self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    # store task gradients\n",
    "    def end_task(self, dataloader, criterion=nn.CrossEntropyLoss()):\n",
    "        self.model.eval()\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            self.model.zero_grad()\n",
    "            loss = criterion(self.model(x), y)\n",
    "            loss.backward()\n",
    "            g = self._flatten_grad()\n",
    "            norm_g = g / (torch.norm(g)+1e-10)\n",
    "            if len(self.S) >= self.max_memory:\n",
    "                self.S.pop(0)\n",
    "            self.S.append(norm_g.detach().clone())\n",
    "\n",
    "# === Cell: prepare CIFAR-10 tasks for SFAO (split) with CNN ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Hyperparameters ===\n",
    "root = './data'\n",
    "num_tasks = 5            \n",
    "num_classes = 10         \n",
    "classes_per_task = num_classes // num_tasks   # = 2\n",
    "batch_size = 32\n",
    "download = True\n",
    "mode = 'split'           # only \"split\"\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# CIFAR-10 mean/std for normalization\n",
    "mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "\n",
    "# === Transforms ===\n",
    "train_transform = transforms.ToTensor()\n",
    "test_transform = transforms.ToTensor()\n",
    "\n",
    "# === Load datasets ===\n",
    "train_ds = datasets.CIFAR10(root=root, train=True, download=download, transform=train_transform)\n",
    "test_ds = datasets.CIFAR10(root=root, train=False, download=download, transform=test_transform)\n",
    "\n",
    "# === Helper: extract subset tensors for a given set of class IDs ===\n",
    "def extract_subset_tensors(dataset, class_list):\n",
    "    targets = np.array(dataset.targets)\n",
    "    mask = np.isin(targets, class_list)\n",
    "    indices = np.nonzero(mask)[0].tolist()\n",
    "    imgs, labs = [], []\n",
    "    for i in indices:\n",
    "        img, lbl = dataset[i]\n",
    "        imgs.append(img)\n",
    "        labs.append(lbl)\n",
    "    return torch.stack(imgs), torch.tensor(labs, dtype=torch.long)\n",
    "\n",
    "# === Build tasks ===\n",
    "train_tasks, test_tasks = [], []\n",
    "for t in range(num_tasks):\n",
    "    cls_start = t * classes_per_task\n",
    "    cls_end = cls_start + classes_per_task\n",
    "    task_classes = list(range(cls_start, cls_end))\n",
    "\n",
    "    x_train, y_train = extract_subset_tensors(train_ds, task_classes)\n",
    "    x_test, y_test = extract_subset_tensors(test_ds, task_classes)\n",
    "\n",
    "    # Normalize\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    # Map labels to 0..(classes_per_task-1)\n",
    "    class_map = {orig: i for i, orig in enumerate(task_classes)}\n",
    "    y_train_mapped = torch.tensor([class_map[int(v)] for v in y_train])\n",
    "    y_test_mapped = torch.tensor([class_map[int(v)] for v in y_test])\n",
    "\n",
    "    # Store datasets (no flattening — CNN expects 3x32x32)\n",
    "    train_tasks.append(TensorDataset(x_train, y_train_mapped))\n",
    "    test_tasks.append(TensorDataset(x_test, y_test_mapped))\n",
    "\n",
    "    print(f\"Task {t}: classes {task_classes[0]}-{task_classes[-1]}, train={len(x_train)}, test={len(x_test)}\")\n",
    "\n",
    "print(f\"Prepared {len(train_tasks)} tasks (mode={mode})\")\n",
    "\n",
    "\"\"\"\n",
    "# === CNN model ===\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):   # 2 classes per task\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x16x16\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 64x8x8\n",
    "        x = x.view(x.size(0), -1)             # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\"\"\"\n",
    "# === WRN model ===\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, 3, stride, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, 3, 1, 1, bias=False)\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = None if self.equalInOut else nn.Conv2d(\n",
    "            in_planes, out_planes, 1, stride, 0, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(x))\n",
    "        shortcut = x if self.equalInOut else self.convShortcut(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.relu2(self.bn2(out))\n",
    "        out = self.conv2(out)\n",
    "        return out + shortcut\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, n, in_planes, out_planes, block, stride):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(\n",
    "                block(in_planes if i == 0 else out_planes,\n",
    "                      out_planes,\n",
    "                      stride if i == 0 else 1)\n",
    "            )\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth=28, widen_factor=10, num_classes=10):\n",
    "        super().__init__()\n",
    "        assert (depth - 4) % 6 == 0\n",
    "        n = (depth - 4) // 6\n",
    "        k = widen_factor\n",
    "        nChannels = [16, 16*k, 32*k, 64*k]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], 3, 1, 1, bias=False)\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], BasicBlock, 1)\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], BasicBlock, 2)\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], BasicBlock, 2)\n",
    "        self.bn = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.relu(self.bn(x))\n",
    "        x = F.adaptive_avg_pool2d(x, 1).view(-1, self.nChannels)\n",
    "        return self.fc(x)\n",
    "\n",
    "# === Training loop ===\n",
    "# sfao = SFAO(SimpleCNN(num_classes=classes_per_task), lr=learning_rate, device=device)\n",
    "sfao = SFAO(WideResNet(depth=28, widen_factor=10, num_classes=classes_per_task),\n",
    "            lr=learning_rate, device=device)\n",
    "accuracy_matrix = np.zeros((num_tasks, num_tasks))\n",
    "\n",
    "def evaluate_task(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x).argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "for task_id, train_dataset in enumerate(train_tasks):\n",
    "    print(f\"\\n=== Training Task {task_id+1} ===\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for x, y in tqdm(train_loader):\n",
    "            epoch_loss += sfao.observe(x, y)\n",
    "        print(f\"Epoch {epoch+1}, Loss={epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate on all seen tasks\n",
    "    for eval_id in range(task_id+1):\n",
    "        test_loader = DataLoader(test_tasks[eval_id], batch_size=batch_size, shuffle=False)\n",
    "        acc = evaluate_task(sfao.model, test_loader)\n",
    "        accuracy_matrix[task_id, eval_id] = acc\n",
    "        print(f\"Accuracy on Task {eval_id+1}: {acc:.3f}\")\n",
    "\n",
    "    sfao.end_task(train_loader)\n",
    "\n",
    "# === Metrics ===\n",
    "ACC = accuracy_matrix[-1].mean()\n",
    "F = np.mean([np.max(accuracy_matrix[:num_tasks-1, j]) - accuracy_matrix[-1, j] for j in range(num_tasks-1)]) if num_tasks>1 else 0.0\n",
    "BWT = np.mean([accuracy_matrix[-1, j] - accuracy_matrix[j, j] for j in range(num_tasks-1)]) if num_tasks>1 else 0.0\n",
    "FWT = np.mean([accuracy_matrix[i, i+1] for i in range(num_tasks-1)]) if num_tasks>1 else 0.0\n",
    "num_params = sum(p.numel() for p in sfao.model.parameters())\n",
    "mem_usage = len(sfao.S) * num_params * 4 / (1024**2)\n",
    "PSM = 0.5*(1-F) + 0.5*max(FWT,0)\n",
    "\n",
    "print(\"=== Metrics (Split CIFAR-10 SFAO) ===\")\n",
    "print(f\"ACC={ACC:.4f}, F={F:.4f}, BWT={BWT:.4f}, FWT={FWT:.4f}, Mem={mem_usage:.2f}MB, PSM={PSM:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(accuracy_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Accuracy')\n",
    "plt.xlabel('Evaluation Task')\n",
    "plt.ylabel('Training Task')\n",
    "plt.title('SFAO Accuracy Matrix (Split CIFAR-10 SFAO)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
